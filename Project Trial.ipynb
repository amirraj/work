{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torchvision.models as models\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from torch.nn.utils.rnn import pack_padded_sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Encoder</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda = torch.device('cuda')\n",
    "resnet = models.resnet152(pretrained=True)\n",
    "modules = list(resnet.children())[:-1]      # delete the last fc layer.\n",
    "resnet = nn.Sequential(*modules)\n",
    "# en_linear = nn.Linear(2048, 1024).to(cuda)\n",
    "# bn = nn.BatchNorm1d(1024, momentum=0.01).to(cuda)\n",
    "\n",
    "class EncoderCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "       super(EncoderCNN, self).__init__()\n",
    "       self.en_linear = nn.Linear(2048, 1024)\n",
    "       self.bn = nn.BatchNorm1d(1024, momentum=0.01)\n",
    "        \n",
    "    def forward(self, images):\n",
    "        features = self.bn(self.en_linear(images))\n",
    "        return features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Pre-Processing</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_names = ['blow','eat','race','sleep','dance','drink','guitar','cook','fight','football']\n",
    "\n",
    "images = []\n",
    "for name in image_names: \n",
    "    input_image = Image.open('E:\\jupyterNotebook\\data\\images\\\\'+name+'.jpg')\n",
    "    preprocess = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "    input_tensor = preprocess(input_image)\n",
    "    images.append(input_tensor)\n",
    "    \n",
    "images = torch.stack(images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Vocabulary</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<end>': 1,\n",
       " '<pad>': 3,\n",
       " '<start>': 0,\n",
       " '<unk>': 2,\n",
       " 'a': 4,\n",
       " 'are': 18,\n",
       " 'bed': 25,\n",
       " 'birthday': 10,\n",
       " 'blowing': 7,\n",
       " 'boy': 12,\n",
       " 'cake': 11,\n",
       " 'candles': 8,\n",
       " 'capsicum': 38,\n",
       " 'cutting': 37,\n",
       " 'dancing': 28,\n",
       " 'drinking': 32,\n",
       " 'fighting': 39,\n",
       " 'floor': 29,\n",
       " 'football': 41,\n",
       " 'from': 9,\n",
       " 'girl': 5,\n",
       " 'guitar': 36,\n",
       " 'having': 13,\n",
       " 'horses': 17,\n",
       " 'is': 6,\n",
       " 'juice': 34,\n",
       " 'man': 22,\n",
       " 'meal': 14,\n",
       " 'men': 40,\n",
       " 'on': 15,\n",
       " 'orange': 33,\n",
       " 'people': 27,\n",
       " 'person': 31,\n",
       " 'playing': 35,\n",
       " 'race': 20,\n",
       " 'racing': 19,\n",
       " 'sleeping': 23,\n",
       " 'table': 16,\n",
       " 'the': 24,\n",
       " 'three': 26,\n",
       " 'track': 21,\n",
       " 'two': 30}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Vocabulary(object):\n",
    "    def __init__(self):\n",
    "        self.word2idx = {}\n",
    "        self.idx2word = {}\n",
    "        self.idx = 0\n",
    "        \n",
    "    def add_word(self, word):\n",
    "        if not word in self.word2idx:\n",
    "            self.word2idx[word] = self.idx\n",
    "            self.idx2word[self.idx] = word\n",
    "            self.idx += 1\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.word2idx)\n",
    "\n",
    "            \n",
    "vocab = Vocabulary()\n",
    "\n",
    "vocab.add_word('<start>')\n",
    "vocab.add_word('<end>')\n",
    "vocab.add_word('<unk>')\n",
    "vocab.add_word('<pad>')\n",
    "\n",
    "captions = ['a girl is blowing candles from a birthday cake',\n",
    "           'a boy is having meal on a table',\n",
    "           'horses are racing on a race track',\n",
    "            'a man is sleeping on the bed',\n",
    "           'three people are dancing on the floor',\n",
    "           'two person are drinking orange juice',\n",
    "           'a girl is playing guitar',\n",
    "            'a man is cutting capsicum',\n",
    "           'two man are fighting',\n",
    "           'men are playing football']\n",
    "\n",
    "for caption in captions:\n",
    "    words = caption.split()\n",
    "    for word in words :\n",
    "        vocab.add_word(word)\n",
    "vocab.word2idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Building Input and Target tensors from the captions</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_list_of_inputs = []\n",
    "tensor_list_of_targets = []\n",
    "\n",
    "for caption in captions:\n",
    "    words = caption.split()\n",
    "    \n",
    "    input_ids = []\n",
    "    target_ids = []\n",
    "    input_ids.append(vocab.word2idx['<start>'])\n",
    "    for word in words :\n",
    "        input_ids.append(vocab.word2idx[word])\n",
    "        target_ids.append(vocab.word2idx[word])\n",
    "    target_ids.append(vocab.word2idx['<end>'])\n",
    "    tensor_list_of_inputs.append(torch.tensor(input_ids))\n",
    "    tensor_list_of_targets.append(torch.tensor(target_ids))\n",
    "    \n",
    "inputs = nn.utils.rnn.pad_sequence(tensor_list_of_inputs,batch_first = True, padding_value = vocab.word2idx['<pad>']).to(cuda)\n",
    "targets = nn.utils.rnn.pad_sequence(tensor_list_of_targets,batch_first = True, padding_value = vocab.word2idx['<pad>']).to(cuda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Decoder, Loss and Optimizer</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed = nn.Embedding(42,512).to(cuda)  # (vocabulary_size,embedding_dimension )\n",
    "lstm1 = nn.LSTMCell(512,1024).to(cuda)     # (input_size, output/hidden size)\n",
    "lstm2 = nn.LSTMCell(1024,1024).to(cuda)\n",
    "linear = nn.Linear(1024,42).to(cuda)    # (hidden_size, vocab_size)\n",
    "loss = nn.CrossEntropyLoss().to(cuda)\n",
    "\n",
    "encoder = EncoderCNN().to(cuda)\n",
    "params = list(lstm1.parameters())+list(lstm2.parameters()) + list(embed.parameters())+ list(linear.parameters())+ list(encoder.parameters())\n",
    "optim = torch.optim.Adam(params, lr= 0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Feature Extraction and Training</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1   tensor(4.1669, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "2   tensor(0.6005, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "3   tensor(0.4770, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "4   tensor(0.2919, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "5   tensor(0.1380, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "6   tensor(0.0761, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "7   tensor(0.0395, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "8   tensor(0.0220, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "9   tensor(0.0140, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "10   tensor(0.0093, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "11   tensor(0.0067, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "12   tensor(0.0046, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "13   tensor(0.0034, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "14   tensor(0.0027, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "15   tensor(0.0021, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "16   tensor(0.0017, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "17   tensor(0.0015, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "18   tensor(0.0013, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "19   tensor(0.0011, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "20   tensor(0.0009, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    resnet_features = resnet(images).squeeze()\n",
    "\n",
    "resnet_features = resnet_features.to(cuda)\n",
    "\n",
    "iteration = 1\n",
    "error = 1000\n",
    "while error > 0.001 :\n",
    "    features = encoder(resnet_features)\n",
    "    embedding = embed(inputs).permute(1,0,2)\n",
    "    hidden1 = features.to(cuda)\n",
    "    cell1 = features.to(cuda)\n",
    "\n",
    "    outputs = []\n",
    "    for i in range(10):\n",
    "        hidden1, cell1 = lstm1(embedding[i], (hidden1,cell1))\n",
    "        hidden2, _ = lstm2(hidden1)\n",
    "        outputs.append(hidden2)\n",
    "        hidden1 = hidden1+features\n",
    "\n",
    "    outputs = torch.stack(outputs)\n",
    "    outputs = outputs.to(cuda)\n",
    "    outputs = linear(outputs)\n",
    "    outputs = outputs.permute(1,2,0)\n",
    "    error = loss(outputs,targets)\n",
    "\n",
    "    lstm1.zero_grad()\n",
    "    lstm2.zero_grad()\n",
    "    embed.zero_grad()\n",
    "    linear.zero_grad()\n",
    "    encoder.zero_grad()\n",
    "\n",
    "    error.backward()\n",
    "    optim.step()\n",
    "    print(iteration,' ',error)\n",
    "    iteration += 1\n",
    "    torch.cuda.empty_cache() \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Test</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.2549, -0.5863,  0.5078,  ...,  0.1732,  0.3482, -0.0046]],\n",
      "       device='cuda:0', grad_fn=<CudnnBatchNormBackward>)\n",
      "torch.Size([10, 42])\n",
      "a\n",
      "man\n",
      "is\n",
      "sleeping\n",
      "on\n",
      "the\n",
      "bed\n",
      "<end>\n"
     ]
    }
   ],
   "source": [
    "name = 'guitar' \n",
    "input_image = Image.open('E:\\jupyterNotebook\\data\\images\\\\'+name+'.jpg')\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "input_tensor = preprocess(input_image)\n",
    "input_tensor = input_tensor.unsqueeze(0)\n",
    "test_feature = resnet(input_tensor).view(1,-1).to(cuda)\n",
    "test_encoder = EncoderCNN().eval().to(cuda)\n",
    "image_feature = test_encoder(test_feature)\n",
    "print(image_feature)\n",
    "test_hidden1 = image_feature.clone().detach()\n",
    "test_cell1 = image_feature.clone().detach()\n",
    "test_embedding = embed(torch.tensor(vocab.word2idx['<start>']).to(cuda)).unsqueeze(0)\n",
    "\n",
    "\n",
    "outputs = []\n",
    "for i in range(10):\n",
    "    test_hidden1, test_cell1 = lstm1(test_embedding,(test_hidden1,test_cell1))\n",
    "    test_hidden2, _ = lstm2(test_hidden1)\n",
    "    out = linear(test_hidden2)\n",
    "    outputs.append(out)\n",
    "    _, predicted = out.max(1)\n",
    "    test_embedding = embed(predicted)\n",
    "    test_hidden1 = test_hidden1+image_feature\n",
    "\n",
    "outputs = torch.cat(outputs)\n",
    "print(outputs.shape)\n",
    "\n",
    "ids = torch.max(outputs,1)[1].tolist()\n",
    "#print(ids)\n",
    "for item in ids:\n",
    "    word = vocab.idx2word[item] \n",
    "    print(word)\n",
    "    if word == '<end>':\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
